name: Security PR Checks

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
  workflow_dispatch:
    inputs:
      preview_url:
        description: "Base URL of the preview environment (https://example.com)"
        required: false
      high_severity_threshold:
        description: "Number of High findings allowed before gating fails"
        required: false
        default: "3"
      enable_kube_hunter:
        description: "Run kube-hunter on the preview cluster"
        required: false
        default: "false"

permissions:
  contents: read
  security-events: write
  pull-requests: write
  checks: write

env:
  PREVIEW_BASE_URL: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.preview_url || vars.PREVIEW_BASE_URL || '' }}
  HIGH_SEVERITY_THRESHOLD: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.high_severity_threshold || vars.SECURITY_HIGH_THRESHOLD || '3' }}
  ENABLE_KUBE_HUNTER: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.enable_kube_hunter || vars.ENABLE_KUBE_HUNTER || 'false' }}
  REPORT_DIR: reports

jobs:
  detect:
    name: Detect technology stack
    runs-on: ubuntu-latest
    outputs:
      has_dotnet: ${{ steps.detect.outputs.has_dotnet }}
      has_maven: ${{ steps.detect.outputs.has_maven }}
      has_gradle: ${{ steps.detect.outputs.has_gradle }}
      has_node: ${{ steps.detect.outputs.has_node }}
      has_docker: ${{ steps.detect.outputs.has_docker }}
      has_kubernetes: ${{ steps.detect.outputs.has_kubernetes }}
      has_terraform: ${{ steps.detect.outputs.has_terraform }}
      has_ansible: ${{ steps.detect.outputs.has_ansible }}
      repository_matrix: ${{ steps.matrix.outputs.matrix }}
      has_languages: ${{ steps.matrix.outputs.has_languages }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - id: detect
        name: Discover languages and artefacts
        run: |
          has_dotnet=$(git ls-files -- '*.sln' | head -n 1)
          has_maven=$(git ls-files -- 'pom.xml' | head -n 1)
          has_gradle=$(git ls-files -- 'build.gradle*' | head -n 1)
          has_node=$(git ls-files -- 'package.json' | head -n 1)
          has_docker=$(git ls-files -- '*Dockerfile' | head -n 1)
          has_kubernetes=$(git ls-files -- '*.yaml' '*.yml' | grep -E '(k8s|kubernetes|helm|chart|manifests|deployment|statefulset)' | head -n 1 || true)
          has_terraform=$(git ls-files -- '*.tf' | head -n 1)
          has_ansible=$(git ls-files -- 'ansible.cfg' | head -n 1)

          echo "has_dotnet=${has_dotnet:+true}" >> "$GITHUB_OUTPUT"
          echo "has_maven=${has_maven:+true}" >> "$GITHUB_OUTPUT"
          echo "has_gradle=${has_gradle:+true}" >> "$GITHUB_OUTPUT"
          echo "has_node=${has_node:+true}" >> "$GITHUB_OUTPUT"
          echo "has_docker=${has_docker:+true}" >> "$GITHUB_OUTPUT"
          echo "has_kubernetes=${has_kubernetes:+true}" >> "$GITHUB_OUTPUT"
          echo "has_terraform=${has_terraform:+true}" >> "$GITHUB_OUTPUT"
          echo "has_ansible=${has_ansible:+true}" >> "$GITHUB_OUTPUT"

      - id: matrix
        name: Build language matrix
        run: |
          python <<'PY'
          import json, os

          def truthy(value: str | None) -> bool:
              return (value or '').lower() == 'true'

          includes = []
          if truthy(os.environ.get('has_dotnet')):
              includes.append({"language": "dotnet"})
          if truthy(os.environ.get('has_maven')):
              includes.append({"language": "java-maven"})
          if truthy(os.environ.get('has_gradle')):
              includes.append({"language": "java-gradle"})
          if truthy(os.environ.get('has_node')):
              includes.append({"language": "node"})

          matrix = {"include": includes}
          outputs = {
              "matrix": json.dumps(matrix),
              "has_languages": "true" if includes else "false",
          }
          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              for key, value in outputs.items():
                  fh.write(f"{key}={value}\n")
          PY
        env:
          has_dotnet: ${{ steps.detect.outputs.has_dotnet }}
          has_maven: ${{ steps.detect.outputs.has_maven }}
          has_gradle: ${{ steps.detect.outputs.has_gradle }}
          has_node: ${{ steps.detect.outputs.has_node }}

  update_databases:
    name: Refresh security feeds
    runs-on: ubuntu-latest
    needs: detect
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Cache Trivy DB
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: ${{ runner.os }}-trivy-db

      - name: Cache Semgrep rules
        uses: actions/cache@v4
        with:
          path: ~/.semgrep
          key: ${{ runner.os }}-semgrep-rules

      - name: Cache Dependency-Check data
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository/org/owasp/dependency-check-data
          key: ${{ runner.os }}-dependency-check-data

      - name: Cache OSV-Scanner data
        uses: actions/cache@v4
        with:
          path: ~/.cache/osv-scanner
          key: ${{ runner.os }}-osv-scanner

      - name: Install tooling packages
        run: |
          sudo apt-get update
          sudo apt-get install -y wget unzip
          go install github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest
          go install github.com/google/osv-scanner/cmd/osv-scanner@latest
        env:
          GOPATH: ${{ runner.temp }}/go
          GOBIN: ${{ runner.temp }}/go/bin
          PATH: ${{ runner.temp }}/go/bin:$PATH

      - name: Update Trivy databases
        uses: aquasecurity/trivy-action@0.20.0
        with:
          download-db-only: true

      - name: Update OSV-Scanner data
        run: |
          mkdir -p ~/.cache/osv-scanner
          osv-scanner --lockfile --recursive . || true
        env:
          PATH: ${{ runner.temp }}/go/bin:$PATH

      - name: Update Nuclei templates
        run: nuclei -update-templates
        env:
          PATH: ${{ runner.temp }}/go/bin:$PATH

      - name: Update Dependency-Check feed
        run: |
          curl -sSL https://github.com/jeremylong/DependencyCheck/releases/latest/download/dependency-check.zip -o /tmp/dependency-check.zip
          unzip -q -o /tmp/dependency-check.zip -d /tmp/dependency-check
          /tmp/dependency-check/dependency-check/bin/dependency-check.sh --updateonly

      - name: Update Semgrep rules
        run: |
          python3 -m pip install --upgrade semgrep
          semgrep --update

  language_scans:
    name: Language build and dependency checks
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    if: needs.detect.outputs.has_languages == 'true'
    strategy:
      matrix: ${{ fromJSON(needs.detect.outputs.repository_matrix) }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup .NET
        if: matrix.language == 'dotnet'
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: Cache NuGet packages
        if: matrix.language == 'dotnet'
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/*.props') }}

      - name: Restore .NET solutions
        if: matrix.language == 'dotnet'
        run: dotnet restore Momentum.sln

      - name: Build .NET
        if: matrix.language == 'dotnet'
        run: dotnet build Momentum.sln --configuration Release --no-restore

      - name: Test .NET
        if: matrix.language == 'dotnet'
        run: dotnet test Momentum.sln --configuration Release --no-build --logger trx --results-directory $REPORT_DIR/dotnet-tests

      - name: Audit .NET packages
        if: matrix.language == 'dotnet'
        run: |
          mkdir -p $REPORT_DIR
          dotnet list package --vulnerable --include-transitive --format json > $REPORT_DIR/dotnet-packages.json || dotnet list package --vulnerable --include-transitive > $REPORT_DIR/dotnet-packages.txt

      - name: Setup Java
        if: startsWith(matrix.language, 'java-')
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: '21'
          cache: ${{ matrix.language == 'java-maven' && 'maven' || matrix.language == 'java-gradle' && 'gradle' || '' }}

      - name: Build with Maven
        if: matrix.language == 'java-maven'
        run: mvn -B -ntp verify

      - name: Build with Gradle
        if: matrix.language == 'java-gradle'
        run: |
          chmod +x gradlew || true
          ./gradlew build

      - name: Restore Dependency-Check cache
        if: startsWith(matrix.language, 'java-')
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository/org/owasp/dependency-check-data
          key: ${{ runner.os }}-dependency-check-data

      - name: Run Dependency-Check scan
        if: startsWith(matrix.language, 'java-')
        uses: dependency-check/Dependency-Check_Action@main
        with:
          args: >-
            --project "Momentum"
            --format "HTML"
            --format "SARIF"
            --out ${{ github.workspace }}/$REPORT_DIR
            --scan ${{ github.workspace }}
            --suppression ${{ github.workspace }}/dependency-check-suppression.xml
          data-directory: ~/.m2/repository/org/owasp/dependency-check-data

      - name: Setup Node
        if: matrix.language == 'node'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: |
            **/package-lock.json
            **/pnpm-lock.yaml
            **/yarn.lock

      - name: Install Node dependencies
        if: matrix.language == 'node'
        run: npm ci --ignore-scripts || npm install --ignore-scripts

      - name: npm audit
        if: matrix.language == 'node'
        run: |
          mkdir -p $REPORT_DIR
          npm audit --json > $REPORT_DIR/npm-audit.json

      - name: Run retire.js
        if: matrix.language == 'node'
        run: npx retire --outputformat json --outputpath $REPORT_DIR/retire.json || true

      - name: Upload language artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: language-${{ matrix.language }}-reports
          path: $REPORT_DIR
          if-no-files-found: ignore

  secret_scan:
    name: Secret scanning
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Gitleaks scan
        uses: zricethezav/gitleaks-action@v2
        with:
          args: >-
            detect --source="${{ github.workspace }}" --redact
            --report-format="sarif" --report-path="${{ github.workspace }}/reports/secrets-gitleaks.sarif"
            --config=${{ github.workspace }}/.gitleaks.toml
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload secret scan results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: secret-scan
          path: $REPORT_DIR
          if-no-files-found: ignore

  semgrep_scan:
    name: Semgrep SAST
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Semgrep (OWASP Top 10)
        uses: semgrep/semgrep-action@v1
        with:
          config: >-
            p/owasp-top-ten
            r/sec-benchmarks
          generateSarif: true
          auditOn: pull_request
          publishSarif: false
        env:
          SEMGREP_RULES: p/owasp-top-ten

      - name: Move Semgrep SARIF
        run: |
          mkdir -p $REPORT_DIR
          find . -name "*semgrep.sarif" -exec mv {} $REPORT_DIR/sast-semgrep.sarif \; || true

      - name: Upload Semgrep results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: semgrep-sast
          path: $REPORT_DIR
          if-no-files-found: ignore

  trivy_fs_scan:
    name: Application filesystem scan
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Prepare report directory
        run: mkdir -p $REPORT_DIR

      - name: Scan repository with Trivy
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: fs
          scanners: vuln,secret,config
          format: sarif
          output: ${{ github.workspace }}/$REPORT_DIR/sast-trivy-fs.sarif
          exit-code: '0'

      - name: Upload Trivy filesystem results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-fs
          path: $REPORT_DIR
          if-no-files-found: ignore

  sbom_and_deps:
    name: SBOM and dependency scanning
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate SBOM (CycloneDX JSON)
        uses: anchore/syft-action@v1
        with:
          source: .
          output: cyclonedx-json=$REPORT_DIR/sbom-cyclonedx.json

      - name: Scan SBOM with Grype
        uses: anchore/scan-action@v3
        with:
          path: $REPORT_DIR/sbom-cyclonedx.json
          fail-build: false
          severity-cutoff: high
          format: sarif
          output: $REPORT_DIR/deps-grype.sarif

      - name: OSV Scanner SBOM validation
        uses: google/osv-scanner-action@v1
        with:
          args: --sbom $REPORT_DIR/sbom-cyclonedx.json --format sarif --output $REPORT_DIR/deps-osv.sarif

      - name: Upload SBOM artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: $REPORT_DIR
          if-no-files-found: ignore

  container_scan:
    name: Container security
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    if: needs.detect.outputs.has_docker == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Discover Dockerfiles
        id: dockerfiles
        run: |
          mapfile -t files < <(git ls-files -- '*Dockerfile')
          printf '%s\n' "files=${files[*]}"
          printf 'files=%s\n' "${files[*]}" >> "$GITHUB_OUTPUT"
          mkdir -p $REPORT_DIR

      - name: Install Trivy CLI
        run: |
          sudo apt-get update
          curl -sSL https://github.com/aquasecurity/trivy/releases/latest/download/trivy_0.50.0_Linux-64bit.tar.gz | tar -xz -C /tmp
          sudo mv /tmp/trivy /usr/local/bin/trivy

      - name: Trivy config scan for Dockerfiles
        if: steps.dockerfiles.outputs.files != ''
        uses: aquasecurity/trivy-action@0.20.0
        with:
          scan-type: config
          scanners: misconfig,secret
          format: sarif
          exit-code: '0'
          scan-ref: ${{ github.workspace }}
          output: ${{ github.workspace }}/$REPORT_DIR/container-trivy-config.sarif

      - name: Build docker images (metadata only)
        id: build-images
        run: |
          mapfile -t files < <(git ls-files -- '*Dockerfile')
          for file in "${files[@]}"; do
            image="ghcr.io/${{ github.repository_owner }}/$(basename "${file%Dockerfile}"):${{ github.sha }}"
            docker build -f "$file" -t "$image" . || true
            echo "$image" >> $REPORT_DIR/docker-images.txt
          done
        env:
          DOCKER_BUILDKIT: 1

      - name: Flag image availability
        run: |
          if [ -s "$REPORT_DIR/docker-images.txt" ]; then
            echo "has_images=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_images=false" >> "$GITHUB_OUTPUT"
          fi
        id: images-flag

      - name: Trivy image scan
        if: steps.images-flag.outputs.has_images == 'true'
        run: |
          while read -r image; do
            [ -z "$image" ] && continue
            trivy image --severity HIGH,CRITICAL --format sarif --output "$REPORT_DIR/container-trivy-${image##*/}.sarif" "$image" || true
          done < $REPORT_DIR/docker-images.txt

      - name: Upload container artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: container-security
          path: $REPORT_DIR
          if-no-files-found: ignore

  iac_scan:
    name: IaC and Kubernetes security
    runs-on: ubuntu-latest
    needs: [detect, update_databases]
    if: >-
      needs.detect.outputs.has_kubernetes == 'true' ||
      needs.detect.outputs.has_terraform == 'true' ||
      needs.detect.outputs.has_ansible == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Prepare report directory
        run: mkdir -p $REPORT_DIR

      - name: kube-linter scan
        if: needs.detect.outputs.has_kubernetes == 'true'
        run: |
          curl -sSL https://github.com/stackrox/kube-linter/releases/latest/download/kube-linter-linux-amd64.tar.gz | tar -xz
          ./kube-linter-linux-amd64/kube-linter lint . --format sarif > $REPORT_DIR/k8s-kube-linter.sarif || true

      - name: tfsec scan
        if: needs.detect.outputs.has_terraform == 'true'
        run: |
          curl -sSL https://github.com/aquasecurity/tfsec/releases/latest/download/tfsec-linux-amd64 -o /tmp/tfsec
          chmod +x /tmp/tfsec
          /tmp/tfsec --format sarif --out $REPORT_DIR/iac-tfsec.sarif || true

      - name: Checkov scan
        if: needs.detect.outputs.has_terraform == 'true'
        run: |
          python3 -m pip install --upgrade checkov
          checkov -d . --output sarif --output-file-path $REPORT_DIR/iac-checkov.sarif || true

      - name: Ansible lint
        if: needs.detect.outputs.has_ansible == 'true'
        run: |
          python3 -m pip install --upgrade ansible ansible-lint
          ansible-lint -f sarif -p --config-file docs/security/ansible-lint.yml > $REPORT_DIR/ansible-lint.sarif || true

      - name: kube-bench (optional)
        if: needs.detect.outputs.has_kubernetes == 'true'
        run: |
          curl -sSL https://github.com/aquasecurity/kube-bench/releases/latest/download/kube-bench_1.7.1_linux_amd64.tar.gz | tar -xz
          ./kube-bench --json > $REPORT_DIR/k8s-kube-bench.json || true

      - name: kube-hunter (opt-in)
        if: needs.detect.outputs.has_kubernetes == 'true' && env.ENABLE_KUBE_HUNTER == 'true'
        run: |
          docker run --rm -it aquasec/kube-hunter --remote ${PREVIEW_BASE_URL} || true

      - name: Upload IaC artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: iac-security
          path: $REPORT_DIR
          if-no-files-found: ignore

  dast:
    name: DAST (ZAP + Nuclei)
    runs-on: ubuntu-latest
    needs: [update_databases]
    if: ${{ (github.event_name == 'workflow_dispatch' && github.event.inputs.preview_url != '') || (github.event_name != 'workflow_dispatch' && vars.PREVIEW_BASE_URL != '') }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare report directory
        run: mkdir -p $REPORT_DIR

      - name: Run OWASP ZAP Baseline
        uses: zaproxy/action-baseline@v0.13.0
        with:
          target: ${{ env.PREVIEW_BASE_URL }}
          cmd_options: -a -m 10 -T 5
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Move ZAP report
        run: |
          mkdir -p $REPORT_DIR
          for file in zap_report.html zap_report.md owasp-zap-scan-report.xml owasp-zap-scan-report.json; do
            if [ -f "$file" ]; then
              base=$(basename "$file")
              mv "$file" "$REPORT_DIR/${base/zap_report/zap-baseline}" || mv "$file" "$REPORT_DIR/$base"
            fi
          done

      - name: Run Nuclei scan
        uses: projectdiscovery/nuclei-action@v1
        with:
          target: ${{ env.PREVIEW_BASE_URL }}
          templates: >-
            cves/
            vulnerabilities/
            misconfiguration/
            exposures/
            default-logins/
            file/technologies/
          exclude: destructive/
          severity: medium,high,critical
          json: true
          output: ${{ github.workspace }}/$REPORT_DIR/dast-nuclei.json

      - name: Convert Nuclei JSON to SARIF
        run: |
          python <<'PY'
          import json, pathlib
          sarif_path = pathlib.Path('${{ github.workspace }}/$REPORT_DIR/dast-nuclei.sarif')
          json_path = pathlib.Path('${{ github.workspace }}/$REPORT_DIR/dast-nuclei.json')
          if not json_path.exists():
              sarif_path.write_text('', encoding='utf-8')
              raise SystemExit(0)
          data = []
          for line in json_path.read_text(encoding='utf-8').splitlines():
              line = line.strip()
              if not line:
                  continue
              try:
                  data.append(json.loads(line))
              except json.JSONDecodeError:
                  continue
          results = []
          for finding in data:
              sev = (finding.get('info', {}).get('severity') or 'medium').lower()
              message = finding.get('info', {}).get('name') or finding.get('template-id', 'nuclei finding')
              description = finding.get('info', {}).get('description') or finding.get('matcher-status', '')
              results.append({
                  'ruleId': finding.get('template-id', 'nuclei'),
                  'level': 'error' if sev in ('critical', 'high') else 'warning' if sev == 'medium' else 'note',
                  'message': {'text': f"{message}: {description}"},
                  'properties': {
                      'severity': sev,
                      'url': finding.get('matched-at'),
                      'tags': finding.get('tags')
                  }
              })
          sarif = {
              '$schema': 'https://json.schemastore.org/sarif-2.1.0.json',
              'version': '2.1.0',
              'runs': [{
                  'tool': {
                      'driver': {
                          'name': 'Nuclei',
                          'informationUri': 'https://nuclei.projectdiscovery.io',
                          'rules': []
                      }
                  },
                  'results': results
              }]
          }
          sarif_path.write_text(json.dumps(sarif, indent=2), encoding='utf-8')
          PY

      - name: Upload DAST artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dast
          path: $REPORT_DIR
          if-no-files-found: ignore

  upload_sarif:
    name: Publish SARIF to code scanning
    runs-on: ubuntu-latest
    needs:
      - language_scans
      - secret_scan
      - semgrep_scan
      - trivy_fs_scan
      - sbom_and_deps
      - container_scan
      - iac_scan
      - dast
    if: always()
    steps:
      - name: Download SARIF artifacts
        uses: actions/download-artifact@v4
        with:
          path: sarif-artifacts

      - name: Upload SARIF files
        run: |
          shopt -s globstar
          find sarif-artifacts -name '*.sarif' -print0 | while IFS= read -r -d '' file; do
            echo "Uploading $file"
            gh code-scanning upload --sarif "$file" --tool-name "security-suite" || true
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  summary:
    name: Summarise findings and gate PR
    runs-on: ubuntu-latest
    needs:
      - detect
      - language_scans
      - secret_scan
      - semgrep_scan
      - trivy_fs_scan
      - sbom_and_deps
      - container_scan
      - iac_scan
      - dast
    if: always()
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: collected

      - name: Aggregate SARIF findings
        id: aggregate
        run: |
          python <<'PY'
          import json, pathlib, os

          report_root = pathlib.Path('collected')
          sarif_files = sorted(report_root.glob('**/*.sarif'))
          summary = {}
          severity_map = ['critical', 'high', 'medium', 'low', 'info']

          def map_severity(value: str | None, level: str | None) -> str:
              if value is None and level is None:
                  return 'info'
              if value is not None:
                  try:
                      numeric = float(value)
                      if numeric >= 9.0:
                          return 'critical'
                      if numeric >= 7.0:
                          return 'high'
                      if numeric >= 4.0:
                          return 'medium'
                      if numeric > 0:
                          return 'low'
                      return 'info'
                  except ValueError:
                      lowered = value.lower()
                      if lowered in severity_map:
                          return lowered
              if level:
                  level = level.lower()
                  if level == 'error':
                      return 'high'
                  if level == 'warning':
                      return 'medium'
                  if level == 'note':
                      return 'low'
              return 'info'

          for sarif_path in sarif_files:
              category = sarif_path.stem.split('-', 1)[0]
              counts = summary.setdefault(category, {sev: 0 for sev in severity_map})
              data = json.load(open(sarif_path, 'r', encoding='utf-8'))
              for run in data.get('runs', []):
                  default_severity = run.get('properties', {}).get('severity')
                  for result in run.get('results', []):
                      sev = map_severity(
                          result.get('properties', {}).get('security-severity')
                          or result.get('properties', {}).get('severity')
                          or default_severity,
                          result.get('level')
                      )
                      counts[sev] += 1

          totals = {sev: sum(category.get(sev, 0) for category in summary.values()) for sev in severity_map}
          high_threshold = int('${{ env.HIGH_SEVERITY_THRESHOLD }}')
          should_fail = totals.get('critical', 0) >= 1 or totals.get('high', 0) >= high_threshold

          summary_md_lines = ["## Security Summary", '', "| Category | Critical | High | Medium | Low | Info |", "| --- | --- | --- | --- | --- | --- |"]
          for category, counts in sorted(summary.items()):
              summary_md_lines.append(f"| {category} | {counts['critical']} | {counts['high']} | {counts['medium']} | {counts['low']} | {counts['info']} |")
          summary_md_lines.append('')
          summary_md_lines.append(f"**Totals**: Critical={totals['critical']}, High={totals['high']} (threshold={high_threshold}), Medium={totals['medium']}, Low={totals['low']}, Info={totals['info']}")
          summary_md_lines.append('')
          summary_md_lines.append(f"Gate status: {'❌ Blocked' if should_fail else '✅ Passing'}")

          report = '\n'.join(summary_md_lines)
          pathlib.Path('${{ env.REPORT_DIR }}').mkdir(exist_ok=True)
          (pathlib.Path('${{ env.REPORT_DIR }}') / 'security-summary.md').write_text(report, encoding='utf-8')
          (pathlib.Path('${{ env.REPORT_DIR }}') / 'security-summary.json').write_text(json.dumps({'categories': summary, 'totals': totals, 'should_fail': should_fail}, indent=2), encoding='utf-8')

          with open('${{ env.REPORT_DIR }}/security-summary.md', 'r', encoding='utf-8') as fh:
              print(fh.read())

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f"should_fail={str(should_fail).lower()}\n")
          PY

      - name: Upload summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: security-summary
          path: $REPORT_DIR

      - name: Post PR comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryPath = process.env.REPORT_DIR + '/security-summary.md';
            let body = fs.readFileSync(summaryPath, 'utf8');
            body += '\n\n' + 'Latest run: ' + new Date().toISOString();
            const header = '<!-- security-pr-checks -->';
            const commentBody = `${header}\n${body}`;
            const { data: comments } = await github.rest.issues.listComments({
              ...context.repo,
              issue_number: context.issue.number,
            });
            const existing = comments.find((comment) => comment.body && comment.body.startsWith(header));
            if (existing) {
              await github.rest.issues.updateComment({
                ...context.repo,
                comment_id: existing.id,
                body: commentBody,
              });
            } else {
              await github.rest.issues.createComment({
                ...context.repo,
                issue_number: context.issue.number,
                body: commentBody,
              });
            }

      - name: Apply attention label when gate fails
        if: steps.aggregate.outputs.should_fail == 'true'
        uses: actions-ecosystem/action-add-labels@v1
        with:
          labels: security:attention

      - name: Fail when gate fails
        if: steps.aggregate.outputs.should_fail == 'true'
        run: |
          echo "Security gate failed due to severity thresholds" >&2
          exit 1

